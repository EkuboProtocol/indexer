import "./config";
import type { EventKey } from "./processor";
import { logger } from "./logger";
import { DAO } from "./dao";
import { Pool } from "pg";
import { throttle } from "tadaaa";
import { EvmStream, Filter } from "@apibara/evm";
import { LOG_PROCESSORS } from "./logProcessors.ts";
import { createClient } from "@apibara/protocol";

const pool = new Pool({
  connectionString: process.env.PG_CONNECTION_STRING,
  connectionTimeoutMillis: 1000,
});

const streamClient = createClient(EvmStream, process.env.APIBARA_URL);

function msToHumanShort(ms: number): string {
  const units = [
    { label: "d", ms: 86400000 },
    { label: "h", ms: 3600000 },
    { label: "min", ms: 60000 },
    { label: "s", ms: 1000 },
    { label: "ms", ms: 1 },
  ];

  const parts: string[] = [];

  for (const { label, ms: unitMs } of units) {
    if (ms >= unitMs) {
      const count = Math.floor(ms / unitMs);
      ms %= unitMs;
      parts.push(`${count}${label}`);
      if (parts.length === 3) break; // Limit to 2 components
    }
  }

  return parts.join(", ") || "0ms";
}

const asyncThrottledRefreshAnalyticalTables = throttle(
  async function (
    since: Date = new Date(
      Date.now() - parseInt(process.env.REFRESH_RATE_ANALYTICAL_VIEWS) * 2,
    ),
  ) {
    const timer = logger.startTimer();
    logger.info("Started refreshing analytical tables", {
      start: timer.start,
      since: since.toISOString(),
    });
    const client = await pool.connect();
    const dao = new DAO(client);
    await dao.beginTransaction();
    await dao.refreshAnalyticalTables({
      since,
    });
    await dao.commitTransaction();
    client.release();
    timer.done({
      message: "Refreshed analytical tables",
      since: since.toISOString(),
    });
  },
  {
    delay: parseInt(process.env.REFRESH_RATE_ANALYTICAL_VIEWS),
    leading: true,
    async onError(err) {
      logger.error("Failed to refresh analytical tables", err);
    },
  },
);

(async function () {
  // first set up the schema
  let databaseStartingCursor;
  {
    const client = await pool.connect();
    const dao = new DAO(client);

    const initializeTimer = logger.startTimer();
    databaseStartingCursor = await dao.initializeSchema();
    initializeTimer.done({
      message: "Initialized schema",
      startingCursor: databaseStartingCursor,
    });
    client.release();
  }

  let lastIsHead = false;

  for await (const message of streamClient.streamData({
    filter: [
      Filter.make({
        header: "on_data",
        logs: LOG_PROCESSORS.map((lp, ix) => ({
          id: ix + 1,
          address: lp.address,
          topics: lp.filter.topics,
          strict: lp.filter.strict,
        })),
      }),
    ],
    finality: "accepted",
    startingCursor: databaseStartingCursor
      ? databaseStartingCursor
      : { orderKey: BigInt(process.env.STARTING_CURSOR_BLOCK_NUMBER ?? 0) },
  })) {
    switch (message._tag) {
      case "heartbeat": {
        logger.info(`Heartbeat`);
        break;
      }

      case "systemMessage": {
        switch (message.systemMessage.output?._tag) {
          case "stderr":
            logger.error(`System message: ${message.systemMessage.output}`);
            break;
          case "stdout":
            logger.info(`System message: ${message.systemMessage.output}`);
            break;
        }
        break;
      }

      case "invalidate": {
        let invalidatedCursor = message.invalidate.cursor;

        if (invalidatedCursor) {
          logger.warn(`Invalidated cursor`, {
            cursor: invalidatedCursor,
          });

          const client = await pool.connect();
          const dao = new DAO(client);

          await dao.beginTransaction();
          await dao.deleteOldBlockNumbers(
            Number(invalidatedCursor.orderKey) + 1,
          );
          await dao.writeCursor(invalidatedCursor);
          await dao.commitTransaction();

          client.release();
        }

        break;
      }

      case "data": {
        const blockProcessingTimer = logger.startTimer();

        const client = await pool.connect();
        const dao = new DAO(client);

        await dao.beginTransaction();

        let deletedCount: number = 0;

        let eventsProcessed: number = 0;
        const isHead = message.data.production === "live";

        for (const block of message.data.data) {
          if (!block) continue;
          const blockNumber = Number(block.header.blockNumber);
          deletedCount += await dao.deleteOldBlockNumbers(blockNumber);

          const blockTime = block.header.timestamp;

          await dao.insertBlock({
            hash: BigInt(block.header.blockHash ?? 0),
            number: block.header.blockNumber,
            time: blockTime,
          });

          for (const event of block.logs) {
            const eventKey: EventKey = {
              blockNumber,
              transactionIndex: event.transactionIndex,
              eventIndex: event.logIndexInTransaction,
              emitter: BigInt(event.address),
              transactionHash: BigInt(event.transactionHash),
            };

            // process each event sequentially through all the event processors in parallel
            // assumption is that none of the event processors operate on the same events, i.e. have the same filters
            // this assumption could be validated at runtime
            await Promise.all(
              event.filterIds.map(async (matchingFilterId) => {
                eventsProcessed++;

                await LOG_PROCESSORS[matchingFilterId - 1].handler(
                  dao,
                  eventKey,
                  {
                    topics: event.topics,
                    data: event.data,
                  },
                );
              }),
            );
          }

          // only write endCursor if cursor is not present
          await dao.writeCursor(message.data.cursor ?? message.data.endCursor);

          const refreshOperational =
            (isHead && (eventsProcessed > 0 || !lastIsHead)) ||
            deletedCount > 0;

          // refresh operational views at the end of the batch
          if (refreshOperational) {
            await dao.refreshOperationalMaterializedView();
          }

          await dao.commitTransaction();

          blockProcessingTimer.done({
            message: `Processed to block`,
            blockNumber,
            isHead,
            refreshOperational,
            eventsProcessed,
            blockTimestamp: blockTime,
            lag: msToHumanShort(
              Math.floor(Date.now() - Number(blockTime.getTime())),
            ),
          });
        }

        client.release();

        if (isHead) {
          asyncThrottledRefreshAnalyticalTables(
            !lastIsHead ? new Date(0) : undefined,
          );
        }

        lastIsHead = isHead;

        break;
      }

      default: {
        logger.error(`Unhandled message type: ${message._tag}`);
        break;
      }
    }
  }
})()
  .then(() => {
    logger.info("Stream closed gracefully");
  })
  .catch((error) => {
    logger.error(error);
    process.exit(1);
  });
